{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoencoderKL\n",
    "\n",
    "This demo is a toy example of how to use MONAI's AutoencoderKL. In particular, it uses\n",
    "the Autoencoder with a Kullback-Leibler regularisation as implemented by Rombach et. al [1].\n",
    "\n",
    "[1] Rombach et. al - [\"High-Resolution Image Synthesis with Latent Diffusion Models\"](https://arxiv.org/pdf/2112.10752.pdf)\n",
    "\n",
    "\n",
    "\n",
    "This tutorial was based on:\n",
    "\n",
    "[Registration Mednist](https://github.com/Project-MONAI/tutorials/blob/main/2d_registration/registration_mednist.ipynb)\n",
    "\n",
    "[Mednist Tutorial](https://github.com/Project-MONAI/tutorials/blob/main/2d_classification/mednist_tutorial.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add buttom with \"Open with Colab\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment using Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"monai-weekly==1.1.dev2239\"\n",
    "!pip install -q matplotlib\n",
    "!pip install -q  torch>=1.7\n",
    "!pip install numpy>=1.17\n",
    "!pip install tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from monai import transforms\n",
    "from monai.apps import MedNISTDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.utils import first, set_determinism\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generative.networks.nets import AutoencoderKL\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility purposes set a seed\n",
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a data directory and download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a `MONAI_DATA_DIRECTORY` variable, where the data will be downloaded. If not\n",
    "specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MedNISTDataset(root_dir=root_dir, section=\"training\", download=True, seed=0)\n",
    "train_datalist = [{\"image\": item[\"image\"]} for item in train_data.data if item[\"class_name\"] == \"Hand\"]\n",
    "# TODO: Add affine\n",
    "# TODO: Add correct flip\n",
    "image_size = 64\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "        transforms.RandAffined(\n",
    "            keys=[\"image\"],\n",
    "            rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],\n",
    "            translate_range=[(-1, 1), (-1, 1)],\n",
    "            scale_range=[(-0.05, 0.05), (-0.05, 0.05)],\n",
    "            spatial_size=[image_size, image_size],\n",
    "            padding_mode=\"zeros\",\n",
    "            prob=0.5,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise examples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3 examples from the training set\n",
    "check_data = first(train_loader)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "for image_n in range(3):\n",
    "    ax[image_n].imshow(check_data[\"image\"][image_n, 0, :, :], cmap=\"gray\")\n",
    "    ax[image_n].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = MedNISTDataset(root_dir=root_dir, section=\"validation\", download=True, seed=0)\n",
    "val_datalist = [{\"image\": item[\"image\"]} for item in train_data.data if item[\"class_name\"] == \"Hand\"]\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ]\n",
    ")\n",
    "val_ds = Dataset(data=val_datalist, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "model = AutoencoderKL(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    n_channels=64,\n",
    "    latent_channels=8,\n",
    "    ch_mult=(1, 2, 3),\n",
    "    num_res_blocks=1,\n",
    "    resolution=(image_size, image_size),\n",
    "    norm_num_groups=16,\n",
    "    with_attention=False,\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_weight = 1e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
    "n_epochs = 50\n",
    "val_interval = 10\n",
    "epoch_loss_list = []\n",
    "val_epoch_loss_list = []\n",
    "intermediary_images = []\n",
    "n_example_images = 4\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        reconstruction, z_mu, z_sigma = model(images)\n",
    "\n",
    "        l1_loss = F.l1_loss(reconstruction.float(), images.float())\n",
    "\n",
    "        kl_loss = 0.5 * torch.sum(z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1, dim=[1, 2, 3])\n",
    "        kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]\n",
    "\n",
    "        # TODO: Add adversarial component\n",
    "        # TODO: Add perceptual loss\n",
    "\n",
    "        loss = l1_loss + kl_weight * kl_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"loss\": epoch_loss / (step + 1),\n",
    "            }\n",
    "        )\n",
    "    epoch_loss_list.append(epoch_loss / (step + 1))\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_step, batch in enumerate(val_loader, start=1):\n",
    "                images = batch[\"image\"].to(device)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                reconstruction, z_mu, z_sigma = model(images)\n",
    "                # get the first sammple from the first validation batch for visualisation\n",
    "                # purposes\n",
    "                if val_step == 1:\n",
    "                    intermediary_images.append(reconstruction[:n_example_images, 0])\n",
    "\n",
    "                l1_loss = F.l1_loss(reconstruction.float(), images.float())\n",
    "\n",
    "                kl_loss = 0.5 * torch.sum(z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1, dim=[1, 2, 3])\n",
    "                kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]\n",
    "\n",
    "                # TODO: Add adversarial component\n",
    "                # TODO: Add perceptual loss\n",
    "\n",
    "                loss = l1_loss + kl_weight * kl_loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= val_step\n",
    "        val_epoch_loss_list.append(val_loss)\n",
    "\n",
    "        print(f\"epoch {epoch + 1} val loss: {val_loss:.4f}\")\n",
    "progress_bar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the training\n",
    "### Visualise the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "val_samples = np.linspace(val_interval, n_epochs, int(n_epochs / val_interval))\n",
    "plt.plot(np.linspace(1, n_epochs, n_epochs), epoch_loss_list, label=\"Train\")\n",
    "plt.plot(val_samples, val_epoch_loss_list, label=\"Validation\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise some reconstruction images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot every evaluation as a new line and example as columns\n",
    "fig, ax = plt.subplots(nrows=len(val_samples), ncols=1, sharey=True)\n",
    "for image_n in range(len(val_samples)):\n",
    "    reconstructions = torch.reshape(intermediary_images[image_n], (image_size * n_example_images, image_size)).T\n",
    "    ax[image_n].imshow(reconstructions.cpu(), cmap=\"gray\")\n",
    "    ax[image_n].set_xticks([])\n",
    "    ax[image_n].set_yticks([])\n",
    "    ax[image_n].set_ylabel(f\"Epoch {val_samples[image_n]:.0f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f0b40d3ec3b7b0e26ca3c7581604d697d7ca3db003a570e1fbb035959308fc97"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
